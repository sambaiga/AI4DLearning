{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 100px; width: 300px\" src=\"images/logo.png\">AI4SG Bootcamp:\n",
    "\n",
    "\n",
    "\n",
    "## Module 2D: Data Scraping\n",
    "\n",
    "\n",
    "**Authors:** Faustine\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Data scraping\n",
    "It is commonly defined as a system where a technology extracts data from a particular codebase or program. Data scraping provides results for a variety of uses and automates aspects of data aggregation.\n",
    "Most common data scraping is *web scraping*, is the process of importing information from a website into a spreadsheet or \n",
    "local file saved on your computer. It’s one of the most efficient ways to get data from the web, and in some cases to channel\n",
    "that data to another website. \n",
    "Data scraping has a vast number of applications – it’s useful in just about any case where data needs to be moved from one place to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uses of data scraping include:\n",
    "<ol start=\"1\">\n",
    "<li>Research for web content/business intelligence </li>\n",
    "<li>Pricing for travel booker sites/price comparison sites </li>\n",
    "<li>Finding sales leads/conducting market research by crawling public data sources (e.g. Yell and Twitter) </li>\n",
    "<li>Sending product data from an e-commerce site to another online vendor (e.g. Google Shopping)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll scrape Goodread's Best Books list:\n",
    "\n",
    "https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1 .\n",
    "\n",
    "We'll walk through scraping the list pages for the book names/urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although many programming languages offer libraries for web information retrieval and analysis, \n",
    "we will be focusing on the Python data analysis ecosystem given its popularity and capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Exploring the Web pages and downloading them</li>\n",
    "<li> Parse the page, extract book urls </li>\n",
    "<li> Parse a book page, extract book properties </li>\n",
    "<li> Set up a pipeline for fetching and parsing</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "Understand the structure of a web page. Use Beautiful soup to scrape content from these web pages.\n",
    "\n",
    "*This lab corresponds to lectures 2, 3 and 4 and maps on to homework 1 and further.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the web pages and downloading them\n",
    "\n",
    "We're going to see the structure of Goodread's best books list. We'll use the Developer tools in chrome, safari and firefox have similar tools available\n",
    "\n",
    "![](images/goodreads1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To getch this page we use the `requests` module. But are we allowed to do this? Lets check:\n",
    "\n",
    "https://www.goodreads.com/robots.txt\n",
    "\n",
    "Yes we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1\n"
     ]
    }
   ],
   "source": [
    "URLSTART=\"https://www.goodreads.com\"\n",
    "BESTBOOKS=\"/list/show/1.Best_Books_Ever?page=\"\n",
    "url = URLSTART+BESTBOOKS+'1'\n",
    "print(url)\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see properties of the page. Most relevant are `status_code` and `text`. The former tells us  if the web-page was found, and if found , ok. (See lecture notes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.status_code # 200 is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html class=\"desktop\\n\">\\n<head>\\n  <title>Best Books Ever (56398 books)</title>\\n\\n<meta content=\\'54,898 books based on 190541 votes: The Hunger Games by Suzanne Collins, Harry Potter and the Order of the Phoenix by J.K. Rowling, To Kill a Mockingbird...\\' name=\\'description\\'>\\n<meta content=\\'telephone=no\\' name=\\'format-detection\\'>\\n<link href=\\'https://www.goodreads.com/list/show/1.Best_Books_Ever\\' rel=\\'canonical\\'>\\n\\n\\n\\n    <script type=\"text/javascript\"> var ue_t0=window.ue_t0||+new Date();\\n </script>\\n  <script type=\"text/javascript\">\\n    (function(e){var c=e;var a=c.ue||{};a.main_scope=\"mainscopecsm\";a.q=[];a.t0=c.ue_t0||+new Date();a.d=g;function g(h){return +new Date()-(h?0:a.t0)}function d(h){return function(){a.q.push({n:h,a:arguments,t:a.d()})}}function b(m,l,h,j,i){var k={m:m,f:l,l:h,c:\"\"+j,err:i,fromOnError:1,args:arguments};c.ueLogError(k);return false}b.skipTrace=1;e.onerror=b;function f(){c.uex(\"ld\")}if(e.addEventListener){e.addEventListener(\"load\",f,false)}else{if(e.attachEvent){e.attachEvent(\"onload\",f)}}a.tag=d(\"tag\");a.log=d(\"log\");a.reset=d(\"rst\");c.ue_csm=c;c.ue=a;c.ueLogError=d(\"err\");c.ues=d(\"ues\");c.uet=d(\"uet\");c.uex=d(\"uex\");c.uet(\"ue\")})(window);(function(e,d){var a=e.ue||{};function c(g){if(!g){return}var f=d.head||d.getElementsByTagName(\"head\")[0]||d.documentElement,h=d.createElement(\"script\");h.async=\"async\";h.src=g;f.insertBefore(h,f.firstChild)}function b(){var k=e.ue_cdn||\"z-ecx.images-amazon.com\",g=e.ue_cdns||\"images-na.ssl-images-amazon.com\",j=\"/images/G/01/csminstrumentation/\",h=e.ue_file||\"ue-full-11e51f253e8ad9d145f4ed644b40f692._V1_.js\",f,i;if(h.indexOf(\"NSTRUMENTATION_FIL\")>=0){return}if(\"ue_https\" in e){f=e.ue_https}else{f=e.location&&e.location.protocol==\"https:\"?1:0}i=f?\"https://\":\"http://\";i+=f?g:k;i+=j;i+=h;c(i)}if(!e.ue_inline){if(a.loadUEFull){a.loadUEFull()}else{b()}}a.uels=c;e.ue=a})(window,document);\\n\\n    if (window.ue && window.ue.tag) { window.ue.tag(\\'list:show:signed_out\\', ue.main_scope);window.ue.tag(\\'list:show:signed_out:desktop\\', ue.main_scope); }\\n  </script>\\n\\n\\n          <script type=\"text/javascript\">\\n        if (window.Mobvious === undefined) {\\n          window.Mobvious = {};\\n        }\\n        window.Mobvious.device_type = \\'desktop\\';\\n        </script>\\n\\n\\n  \\n<script src=\"https://s.gr-assets.com/assets/webfontloader-95615182ff3a65e7ea91408cc85f05fb.js\"></script>\\n<script>\\n//<![CDATA[\\n\\n  WebFont.load({\\n    classes: false,\\n    custom: {\\n      families: [\"Lato:n4,n7,i4\", \"Merriweather:n4,n7,i4\"],\\n      urls: [\"https://s.gr-assets.com/assets/gr/fonts-e256f84093cc13b27f5b82343398031a.css\"]\\n    }\\n  });\\n\\n//]]>\\n</script>\\n\\n  <link rel=\"stylesheet\" media=\"all\" href=\"https://s.gr-assets.com/assets/goodreads-6826f801189f342b8e80c982d6c3cd83.css\" />\\n\\n    <style type=\"text/css\" media=\"screen\">\\n    .bigTabs {\\n      margin-bottom: 10px;\\n    }\\n\\n    .list_read{\\n      background-color: #D7D2C4;\\n      float: left;\\n    }\\n  </style>\\n\\n\\n  <link rel=\"stylesheet\" media=\"screen\" href=\"https://s.gr-assets.com/assets/common_images-670d97636259cafc355c94fc43e871d7.css\" />\\n\\n  <script src=\"https://s.gr-assets.com/assets/desktop/libraries-90e8f799b69f7d309e40ac6fb894a824.js\"></script>\\n  <script src=\"https://s.gr-assets.com/assets/application-8297c00735d7654c056f50fb255deb60.js\"></script>\\n\\n    <script>\\n  //<![CDATA[\\n    var gptAdSlots = gptAdSlots || [];\\n    var googletag = googletag || {};\\n    googletag.cmd = googletag.cmd || [];\\n    (function() {\\n      var gads = document.createElement(\"script\");\\n      gads.async = true;\\n      gads.type = \"text/javascript\";\\n      var useSSL = \"https:\" == document.location.protocol;\\n      gads.src = (useSSL ? \"https:\" : \"http:\") +\\n      \"//www.googletagservices.com/tag/js/gpt.js\";\\n      var node = document.getElementsByTagName(\"script\")[0];\\n      node.parentNode.insertBefore(gads, node);\\n    })();\\n    // page settings\\n  //]]>\\n</script>\\n<script>\\n  //<![CDATA[\\n    googletag.cmd.push(function() {\\n      googletag.pubads().setTargeting(\"sid\", \"osid.dcad2517d9bf82665499beb787cee583\");\\n    googletag.pubads().setTargeting(\"grsession\", \"osid.dcad2517d9bf82665499beb787cee583\");\\n    googletag.pubads().setTargeting(\"surface\", \"desktop\");\\n    googletag.pubads().setTargeting(\"signedin\", \"false\");\\n    googletag.pubads().setTargeting(\"gr_author\", \"false\");\\n    googletag.pubads().setTargeting(\"author\", []);\\n    googletag.pubads().setTargeting(\"shelf\", [\"best\",\"earliestlist\",\"favourites\",\"fiction\",\"writer\"]);\\n    googletag.pubads().setTargeting(\"gtargeting\", \"1z141z5\");\\n    googletag.pubads().setTargeting(\"resource\", \"List_1\");\\n      googletag.pubads().enableAsyncRendering();\\n      googletag.pubads().enableSingleRequest();\\n      googletag.pubads().collapseEmptyDivs(true);\\n      googletag.pubads().disableInitialLoad();\\n      googletag.enableServices();\\n    });\\n  //]]>\\n</script>\\n<script>\\n  //<![CDATA[\\n    ! function(a9, a, p, s, t, A, g) {\\n      if (a[a9]) return;\\n    \\n      function q(c, r) {\\n        a[a9]._Q.push([c, r])\\n      }\\n      a[a9] = {\\n      init: function() {'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.text[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a loop to fetch 2 pages of \"best-books\" from goodreads. Notice the use of a format string. This is an example of old-style python format strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTW files/page01.html\n",
      "FTW files/page02.html\n"
     ]
    }
   ],
   "source": [
    "URLSTART=\"https://www.goodreads.com\"\n",
    "BESTBOOKS=\"/list/show/1.Best_Books_Ever?page=\"\n",
    "for i in range(1,3):\n",
    "    bookpage=str(i)\n",
    "    stuff=requests.get(URLSTART+BESTBOOKS+bookpage)\n",
    "    filetowrite=\"files/page\"+ '%02d' % i + \".html\"\n",
    "    print(\"FTW\", filetowrite)\n",
    "    fd=open(filetowrite,\"w\")\n",
    "    fd.write(stuff.text)\n",
    "    fd.close()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse the page, extract book urls\n",
    "\n",
    "Notice how we do file input-output, and use beautiful soup in the code below. The `with` construct ensures that the file being read is closed, something we do explicitly for the file being written. We look for the elements with class `bookTitle`, extract the urls, and write them into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTW files/page01.html\n",
      "['/book/show/2767052-the-hunger-games', '/book/show/2.Harry_Potter_and_the_Order_of_the_Phoenix', '/book/show/2657.To_Kill_a_Mockingbird', '/book/show/1885.Pride_and_Prejudice', '/book/show/41865.Twilight', '/book/show/19063.The_Book_Thief', '/book/show/7613.Animal_Farm', '/book/show/11127.The_Chronicles_of_Narnia', '/book/show/30.J_R_R_Tolkien_4_Book_Boxed_Set', '/book/show/18405.Gone_with_the_Wind']\n",
      "FTW files/page02.html\n",
      "['/book/show/43763.Interview_with_the_Vampire', '/book/show/153747.Moby_Dick_or_the_Whale', '/book/show/5.Harry_Potter_and_the_Prisoner_of_Azkaban', '/book/show/4989.The_Red_Tent', '/book/show/37435.The_Secret_Life_of_Bees', '/book/show/7171637-clockwork-angel', '/book/show/2187.Middlesex', '/book/show/6.Harry_Potter_and_the_Goblet_of_Fire', '/book/show/16299.And_Then_There_Were_None', '/book/show/49552.The_Stranger']\n"
     ]
    }
   ],
   "source": [
    "bookdict={}\n",
    "for i in range(1,3):\n",
    "    books=[]\n",
    "    stri = '%02d' % i\n",
    "    filetoread=\"files/page\"+ stri + '.html'\n",
    "    print(\"FTW\", filetoread)\n",
    "    with open(filetoread) as fdr:\n",
    "        data = fdr.read()\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    for e in soup.select('.bookTitle'):\n",
    "        books.append(e['href'])\n",
    "    print(books[:10])\n",
    "    bookdict[stri]=books\n",
    "    fd=open(\"files/list\"+stri+\".txt\",\"w\")\n",
    "    fd.write(\"\\n\".join(books))\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is George Orwell's 1984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/book/show/43763.Interview_with_the_Vampire'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookdict['02'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lets go look at the first URLs on both pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/goodreads2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse a book page, extract book properties\n",
    "\n",
    "Ok so now lets dive in and get one of these these files and parse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.goodreads.com/book/show/43763.Interview_with_the_Vampire'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "furl=URLSTART+bookdict['02'][0]\n",
    "furl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/goodreads3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "fstuff=requests.get(furl)\n",
    "print(fstuff.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=BeautifulSoup(fstuff.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Interview with the Vampire (The Vampire Chronicles, #1)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.select(\"meta[property='og:title']\")[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get everything we want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title Interview with the Vampire (The Vampire Chronicles, #1) \n",
      " isbn 9780345476876 \n",
      " type books.book \n",
      " author https://www.goodreads.com/author/show/7577.Anne_Rice \n",
      " ratingCount 442591 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "d=BeautifulSoup(fstuff.text, 'html.parser')\n",
    "print(\n",
    "\"title\", d.select_one(\"meta[property='og:title']\")['content'],\"\\n\",\n",
    "\"isbn\", d.select(\"meta[property='books:isbn']\")[0]['content'],\"\\n\",\n",
    "\"type\", d.select(\"meta[property='og:type']\")[0]['content'],\"\\n\",\n",
    "\"author\", d.select(\"meta[property='books:author']\")[0]['content'],\"\\n\",\n",
    "#\"average rating\", d.select_one(\"span.average\").text,\"\\n\",\n",
    "\"ratingCount\", d.select(\"meta[itemprop='ratingCount']\")[0][\"content\"],\"\\n\",\n",
    "#\"reviewCount\", d.select_one(\"span.count\")[\"title\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we know what to do, lets wrap our fetching into a proper script. So that we dont overwhelm their servers, we will only fetch 5 from each page, but you get the idea...\n",
    "\n",
    "We'll segue of a bit to explore new style format strings. See https://pyformat.info for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'list03.txt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"list{:0>2}.txt\".format(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"4\"\n",
    "b = 4\n",
    "class Four:\n",
    "    def __str__(self):\n",
    "        return \"Fourteen\"\n",
    "c=Four()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The hazy cat jumped over the 4 and 4 and Fourteen'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The hazy cat jumped over the {} and {} and {}\".format(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up a pipeline for fetching and parsing\n",
    "\n",
    "Ok lets get back to the fetching..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTW files/1_2767052-the-hunger-games.html\n",
      "FTW files/1_2.Harry_Potter_and_the_Order_of_the_Phoenix.html\n",
      "FTW files/1_2657.To_Kill_a_Mockingbird.html\n",
      "FTW files/1_1885.Pride_and_Prejudice.html\n",
      "FTW files/1_41865.Twilight.html\n",
      "FTW files/2_43763.Interview_with_the_Vampire.html\n",
      "FTW files/2_153747.Moby_Dick_or_the_Whale.html\n",
      "FTW files/2_5.Harry_Potter_and_the_Prisoner_of_Azkaban.html\n",
      "FTW files/2_4989.The_Red_Tent.html\n",
      "FTW files/2_37435.The_Secret_Life_of_Bees.html\n",
      "['files/1_2767052-the-hunger-games.html', 'files/1_2.Harry_Potter_and_the_Order_of_the_Phoenix.html', 'files/1_2657.To_Kill_a_Mockingbird.html', 'files/1_1885.Pride_and_Prejudice.html', 'files/1_41865.Twilight.html', 'files/2_43763.Interview_with_the_Vampire.html', 'files/2_153747.Moby_Dick_or_the_Whale.html', 'files/2_5.Harry_Potter_and_the_Prisoner_of_Azkaban.html', 'files/2_4989.The_Red_Tent.html', 'files/2_37435.The_Secret_Life_of_Bees.html']\n"
     ]
    }
   ],
   "source": [
    "fetched=[]\n",
    "for i in range(1,3):\n",
    "    with open(\"files/list{:0>2}.txt\".format(i)) as fd:\n",
    "        counter=0\n",
    "        for bookurl_line in fd:\n",
    "            if counter > 4:\n",
    "                break\n",
    "            bookurl=bookurl_line.strip()\n",
    "            stuff=requests.get(URLSTART+bookurl)\n",
    "            filetowrite=bookurl.split('/')[-1]\n",
    "            filetowrite=\"files/\"+str(i)+\"_\"+filetowrite+\".html\"\n",
    "            print(\"FTW\", filetowrite)\n",
    "            fd=open(filetowrite,\"w\", encoding='utf-8')\n",
    "            fd.write(stuff.text)\n",
    "            fd.close()\n",
    "            fetched.append(filetowrite)\n",
    "            time.sleep(2)\n",
    "            counter=counter+1\n",
    "            \n",
    "print(fetched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we are off to parse each one of the html pages we fetched. We have provided the skeleton of the code and the code to parse the year, since it is a bit more complex...see the difference in the screenshots above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "yearre = r'\\d{4}'\n",
    "def get_year(d):\n",
    "    if d.select_one(\"nobr.greyText\"):\n",
    "        return d.select_one(\"nobr.greyText\").text.strip().split()[-1][:-1]\n",
    "    else:\n",
    "        thetext=d.select(\"div#details div.row\")[1].text.strip()\n",
    "        rowmatch=re.findall(yearre, thetext)\n",
    "        if len(rowmatch) > 0:\n",
    "            rowtext=rowmatch[0].strip()\n",
    "        else:\n",
    "            rowtext=\"NA\"\n",
    "        return rowtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "Your job is to fill in the code to get the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genres(d):\n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "listofdicts=[]\n",
    "for filetoread in fetched:\n",
    "    print(filetoread)\n",
    "    td={}\n",
    "    with open(filetoread) as fd:\n",
    "        datext = fd.read()\n",
    "    d=BeautifulSoup(datext, 'html.parser')\n",
    "    td['title']=d.select_one(\"meta[property='og:title']\")['content']\n",
    "    td['isbn']=d.select_one(\"meta[property='books:isbn']\")['content']\n",
    "    td['booktype']=d.select_one(\"meta[property='og:type']\")['content']\n",
    "    td['author']=d.select_one(\"meta[property='books:author']\")['content']\n",
    "    td['rating']=d.select_one(\"span.average\").text\n",
    "    td['ratingCount']=d.select_one(\"meta[itemprop='ratingCount']\")[\"content\"]\n",
    "    td['reviewCount']=d.select_one(\"span.count\")[\"title\"]\n",
    "    td['year'] = get_year(d)\n",
    "    td['file']=filetoread\n",
    "    glist = get_genres(d)\n",
    "    td['genres']=\"|\".join(glist)\n",
    "    listofdicts.append(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofdicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets write all this stuff into a csv file which we will use to do analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(listofdicts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"files/meta.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Now we have used Beautiful soup to scrape content from Goodreads website. \n",
    "\n",
    "Whether or not you intend to use data scraping in your work, it’s advisable to still educate yourself on the subject, as it is most likely to become even more important in the next few years.\n",
    "\n",
    "There are now data scraping AI on the market that can use machine learning to keep on getting better at recognising inputs which only humans have traditionally been able to interpret – like images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
